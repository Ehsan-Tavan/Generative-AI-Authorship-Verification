<div align="center">
<h2>
BinocularsLLM, Fusing Binoculars’ Insight with the Proficiency of Large Language Models for Machine-Generated Text Detection
</h2>
</div>

 <div align="center">
<b>Ehsan Tavan</b><sup>1</sup>,
<b>Maryam Najafi</b><sup>2</sup>
</div>

<div align="center">
<sup>1</sup>NLP Department, Part AI Research Center, Tehran, Iran
</div>
<div align="center">
<sup>2</sup>Department of Computer Science and Information Systems, University of Limerick, Ireland
</div>
 

## Overview


This repository hosts the source code for the **BinocularsLLM Framework**. 
Our framework achieved top performance in the [Voight-Kampff Generative AI 
Authorship Verification](https://pan.webis.de/clef24/pan24-web/generated-content-analysis.html) task of PAN 2024. [[Paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=oY-ufO0AAAAJ&citation_for_view=oY-ufO0AAAAJ:Y0pCki6q_DkC)]

## Getting Started

### Installation
To use the BinocularsLLM Framework, clone this repository and install 
the required packages listed in `requirements.txt` using pip. 
The code is developed and tested with Python 3.10. 
Execute the following commands to set up the environment:

```bash
git clone https://github.com/Ehsan-Tavan/Generative-AI-Authorship-Verification.git
cd Generative-AI-Authorship-Verification
pip install -r requirements.txt
```


### Download Required Models

To run the BinocularsLLM Framework, you'll need to download the following language models:

- [Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b)
- [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [Falcon-7b](https://huggingface.co/tiiuae/falcon-7b)
- [Falcon-7b-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)

Additionally, download the custom LoRA weights for these models:

- [Generative-AV-Mistral-v0.1-7b](https://huggingface.co/Ehsan-Tavan/Generative-AV-Mistral-v0.1-7b)
- [Generative-AV-LLaMA-2-7b](https://huggingface.co/Ehsan-Tavan/Generative-AV-LLaMA-2-7b)

### Running Inference

Before running the framework, ensure that all models and LoRA weights are downloaded and saved to accessible locations on your system.

To run the framework, execute `inferencer.py` with the required arguments. Below is an example command structure:

```bash
python inferencer.py \
  --llama_model_path="Path to Llama-2-7b" \
  --llama_peft_model_path="Path to Generative-AV-LLaMA-2-7b" \
  --mistral_model_path="Path to Mistral-7B-v0.1" \
  --mistral_peft_model_path="Path to Generative-AV-Mistral-v0.1-7b" \
  --observer_name_or_path="Path to Falcon-7b" \
  --performer_name_or_path="Path to Falcon-7b-Instruct" \
  --outputDir="Path to output .jsonl file" \
  --inputDataset="Path to input .jsonl file"
```

### Instructions for Input and Output Files

#### Input File

- **Format**: The input will be provided as a JSON Lines (JSONL) file, with each line representing a JSON object.
- **Content**: Each line contains a pair of texts that need to be analyzed. The structure of each JSON object is as follows:

  ```json
  {
    "id": "unique_identifier",
    "text1": "first_text_to_analyze",
    "text2": "second_text_to_analyze"
  }

- **id**: A unique identifier for the text pair (string).
- **text1**: The first text in the pair (string).
- **text2**: The second text in the pair (string).

#### Example:
```json
    {"id": "iixcWBmKWQqLAwVXxXGBGg", "text1": "Sample text 1", "text2": "Sample text 2"}
    {"id": "y12zUebGVHSN9yiL8oRZ8Q", "text1": "Another text 1", "text2": "Another text 2"}
```
#### Output File

- **Format**: The output should also be a JSON Lines (JSONL) file, where each line represents the analysis result for the corresponding text pair.

- **Content**: Each line contains the unique identifier from the input file and the result of the analysis (is_human score). The structure of each JSON object is as follows:

```json
    {
      "id": "unique_identifier",
      "is_human": predicted score
    }
```

- **id**: The unique identifier corresponding to the text pair in the input file (string).
- **is_human**: The predicted probability that the text pair was generated by a human (float between 0 and 1, where 1.0 indicates high probability of being human-generated, and 0.0 indicates low probability).

#### Example:

```json
        {"id": "iixcWBmKWQqLAwVXxXGBGg", "is_human": 1.0}
        {"id": "y12zUebGVHSN9yiL8oRZ8Q", "is_human": 0.3}
```


## Push to TIRA

You can push this software to tira via:

```
tira-run \
    --image pan24-generative-authorship:latest \
    --input-dataset generative-ai-authorship-verification-panclef-2024/pan24-generative-authorship-tiny-smoke-20240417-training \
    --mount-hf-model meta-llama/Llama-2-7b-hf Ehsan-Tavan/Generative-AV-LLaMA-2-7b mistralai/Mistral-7B-v0.1 Ehsan-Tavan/Generative-AV-Mistral-v0.1-7b tiiuae/falcon-7b tiiuae/falcon-7b-instruct \
    --command 'python /app/src/inferencer.py --llama_model_path=meta-llama/Llama-2-7b-hf --llama_peft_model_path=/root/.cache/huggingface/hub/models--Ehsan-Tavan--Generative-AV-LLaMA-2-7b/snapshots/3df014e07f262611b4eb9daa8a75cc486702b138/ --mistral_model_path=mistralai/Mistral-7B-v0.1 --mistral_peft_model_path=/root/.cache/huggingface/hub/models--Ehsan-Tavan--Generative-AV-Mistral-v0.1-7b/snapshots/593a0ade0090e3988824f7c05779360d24c5048e/ --observer_name_or_path=/root/.cache/huggingface/hub/models--tiiuae--falcon-7b/snapshots/898df1396f35e447d5fe44e0a3ccaaaa69f30d36/ --performer_name_or_path=/root/.cache/huggingface/hub/models--tiiuae--falcon-7b-instruct/snapshots/cf4b3c42ce2fdfe24f753f0f0d179202fea59c99/ --outputDir=$outputDir/results.jsonl --inputDataset=$inputDataset/dataset.jsonl' \
    --push true
```


## BinocularsLLM Framework Structure


    .
    ├── data/                           # Directory containing dataset files
    │
    ├── src/                            # Main source code directory
    │     ├── binoculars/               # Core logic and implementation of the Binoculars model
    │     ├── configuration/            # Configuration files and settings for the framework
    │     ├── data_loader/              # Scripts and modules for loading datasets 
    │     ├── data_preparation/         # Code for preparing and preprocessing data
    │     ├── dataset/                  # Dataset-related utilities and classes
    │     ├── inference/                # Inference scripts for model predictions
    │     ├── models/                   # Model architecture definitions and implementations
    │     ├── utils/                    # Utility functions and helper scripts 
    │     ├── inferencer.py             # Script for running inference using the trained model
    │     ├── instruction_trainer.py    # Script for training the model with instructional data
    │     ├── llm_trainer.py            # Script for fine-tuning the LLM with a classification head
    │     ├── prepare_training_data.py  # Script for preparing data for training
    │   
    ├── Dockerfile                      # Docker configuration file for containerizing the framework
    ├── README.md                       # Documentation and usage instructions
    └── requirements.txt                # List of dependencies required to run the framework
